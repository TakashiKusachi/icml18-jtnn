{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習プログラム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path通し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データプレプロセス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocablary作成\n",
    "すでにvocab.txtが作成済みである場合不用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user aisiars\n",
      " ··········\n"
     ]
    }
   ],
   "source": [
    "from fast_jtnn.mol_tree import MolTree\n",
    "from getpass import getpass,getuser\n",
    "\n",
    "import mysql\n",
    "from mysql import connector\n",
    "import warnings\n",
    "\n",
    "# SQL server profile\n",
    "host = \"localhost\"\n",
    "user = None\n",
    "passwd = None\n",
    "port = 3306\n",
    "database=\"chemoinfo\"\n",
    "\n",
    "# \n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "# get massbank data from SQL server\n",
    "try:\n",
    "    if not isinstance(user,str):\n",
    "        user = raw_input(\"user\")\n",
    "    if not isinstance(passwd,str):\n",
    "        passwd = getpass()\n",
    "    connect = connector.connect(host=host,user=user,password=passwd,port=port,database=database)\n",
    "    cursor = connect.cursor()\n",
    "    cursor.execute(\"\"\"select smiles from massbank where ms_type=\"MS\" and instrument_type=\"EI-B\" and smiles<>'N/A'; \"\"\")\n",
    "    smiles_list = cursor.fetchall()\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Something went wrong: {}\".format(e))\n",
    "    sys.exit(1)\n",
    "finally:\n",
    "    if passwd : del passwd\n",
    "    if connect: connect.close()\n",
    "    if cursor: cursor.close()\n",
    "\n",
    "# create vocablary\n",
    "succes = 0\n",
    "fault = 0\n",
    "cset = set()\n",
    "for one in smiles_list:\n",
    "    try:\n",
    "        mol = MolTree(one[0])\n",
    "    except AttributeError as e:\n",
    "        warnings.warn(\"Entered An SMILES that does not meet the rules\")\n",
    "        continue\n",
    "    for c in mol.nodes:\n",
    "        cset.add(c.smiles)\n",
    "\n",
    "# write vocab\n",
    "with open(VOCAB_FILE,\"w\") as f:\n",
    "    for one in cset:\n",
    "        f.write(one+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! mkdir vae_model/\n",
    "%run ../fast_molvae/vae_train.py --train processed --vocab ./MS_vocab.txt --save_dir vae_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab,datasetのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user aisiars\n",
      " ··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 8481/11200 [23:04<05:35,  8.10it/s]"
     ]
    }
   ],
   "source": [
    "from fast_jtnn import *\n",
    "from MS_PredictModel import MS_Dataset\n",
    "\n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(VOCAB_FILE,\"r\")]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "MS_Dataset.QUERY = \"\"\"select smiles,file_path from massbank where ms_type=\"MS\" and instrument_type=\"EI-B\" and smiles<>'N/A'; \"\"\"\n",
    "dataset = MS_Dataset(vocab=vocab,host=\"localhost\",database=\"chemoinfo\",batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(365, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(365, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=365, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder(\n",
      "  (W1): Linear(in_features=866, out_features=56, bias=True)\n",
      "  (W2): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n",
      "Model #Params: 367K\n"
     ]
    }
   ],
   "source": [
    "from ms_encoder import ms_peak_encoder\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "hidden_size = 100\n",
    "latent_size = 56\n",
    "depthT = 20\n",
    "depthG = 3\n",
    "\n",
    "dec_model = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG).cuda()\n",
    "print dec_model\n",
    "enc_model = ms_peak_encoder(dataset.max_spectrum_size,latent_size).cuda()\n",
    "print enc_model\n",
    "\n",
    "for param in dec_model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)\n",
    "load_model = \"./vae_model/model.iter-70000\"\n",
    "dec_model.load_state_dict(torch.load(load_model))\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in dec_model.parameters()]) / 1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オプティマイザの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = optim.Adam(enc_model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.43it/s]/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "99it [00:25,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:26,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100] , Word: 32.14, Topo: 85.34, Assm: 80.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "198it [00:52,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:56,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] , Word: 34.05, Topo: 86.37, Assm: 83.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "297it [01:19,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "300it [01:23,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300] , Word: 35.90, Topo: 86.84, Assm: 84.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "396it [01:48,  5.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [01:50,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400] , Word: 36.68, Topo: 87.27, Assm: 85.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "495it [02:14,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [02:17,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500] , Word: 36.60, Topo: 87.12, Assm: 84.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "594it [02:39,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [02:41,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600] , Word: 35.90, Topo: 87.16, Assm: 85.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "693it [03:03,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [03:04,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700] , Word: 35.80, Topo: 87.15, Assm: 83.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "792it [03:29,  6.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "800it [03:34,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800] , Word: 35.64, Topo: 87.10, Assm: 85.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "891it [03:56,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900it [04:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900] , Word: 34.55, Topo: 87.87, Assm: 85.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "990it [04:22,  4.73it/s]"
     ]
    }
   ],
   "source": [
    "from MS_PredictModel import ms_peak_encoder,MS_Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "if pbar: pbar = tqdm()\n",
    "def training(max_epoch = 100):\n",
    "    global pbar\n",
    "    total_step = 0\n",
    "    meters = np.zeros(3)\n",
    "    for epoch in range(max_epoch):\n",
    "        print(\"epoch : \",epoch)\n",
    "        for batch in dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            total_step+=1\n",
    "            pbar.update(1)\n",
    "            enc_model.zero_grad()\n",
    "            h = enc_model(x,y)\n",
    "            tree_vec = h[:,:h.shape[1]/2]\n",
    "            mol_vec  = h[:,h.shape[1]/2:]\n",
    "            _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "            word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "            assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "            total_loss = word_loss+topo_loss+assm_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            meters = meters + np.array([word_acc * 100, topo_acc * 100, assm_acc * 100])\n",
    "            if total_step % 100 == 0:\n",
    "                meters /= 100\n",
    "                print \"[%d] , Word: %.2f, Topo: %.2f, Assm: %.2f\" % (total_step,meters[0], meters[1], meters[2])\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "            if total_step % 100 == 0:\n",
    "                torch.save(enc_model.state_dict(), \"./enc_model\" + \"/model.iter-\" + str(total_step))\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "try:\n",
    "    training(10)\n",
    "except RuntimeError as e:\n",
    "    import pdb; pdb.set_trace()\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fast_jtnn.datautils import tensorize\n",
    "import random\n",
    "\n",
    "random.shuffle(dataset.dataset)\n",
    "test = [[one[2]] for one in dataset.dataset]\n",
    "for one in test:\n",
    "    print(len(one),one[0].smiles)\n",
    "    tensorize(one,vocab,assm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test1', 'C[CH3:5]', ['C[CH3:5]'])\n",
      "('test1', 'O=[CH2:5]', ['O=[CH2:5]'])\n",
      "('test1', 'c1cc[c:3]([CH3:5])cc1', ['c1cc[c:3]([CH3:5])cc1'])\n",
      "('test1', 'c1ccc([CH3:3])cc1', ['c1ccc([CH3:3])cc1'])\n",
      "('test1', 'C[C:5](=O)[CH3:3]', ['C[C:5](=O)[CH3:3]'])\n",
      "('CC', True, 1)\n",
      "('C=O', True, 1)\n",
      "('CC', False, 1)\n",
      "('C1=CC=CC=C1', True, 1)\n",
      "('C', False, 1)\n",
      "[]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7aa45a25b690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mbatch_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mjtmpn_holder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJTMPN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmess_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jupyterclient/jupyter_workspace/workspace/icml18-jtnn/fast_jtnn/jtmpn.pyc\u001b[0m in \u001b[0;36mtensorize\u001b[0;34m(cand_batch, mess_dict)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mtotal_bonds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_bonds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mfatoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfatoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mfbonds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfbonds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0magraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_atoms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_NB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from fast_jtnn import MolTree\n",
    "\n",
    "smiles = \"CC(=O)c1ccccc1\"\n",
    "#smiles = \"CCCCOC(=O)c(c1)c(ccc1)C(=O)OCCCC\"\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "Draw.MolToImage(mol)\n",
    "\n",
    "mol_tree = MolTree(smiles) # convert smiles to MolTree\n",
    "mol_tree.recover()\n",
    "assm = True\n",
    "if assm:\n",
    "    mol_tree.assemble()\n",
    "    for node in mol_tree.nodes:\n",
    "        print(\"test1\",node.label,node.cands)\n",
    "        if node.label not in node.cands:\n",
    "            print(\"test2\",node.label)\n",
    "            node.cands.append(node.label)\n",
    "\n",
    "del mol_tree.mol\n",
    "for node in mol_tree.nodes:\n",
    "    del node.mol\n",
    "\n",
    "def set_batch_nodeID(mol_batch, vocab):\n",
    "    tot = 0\n",
    "    for mol_tree in mol_batch:\n",
    "        for node in mol_tree.nodes:\n",
    "            node.idx = tot\n",
    "            node.wid = vocab.get_index(node.smiles)\n",
    "            tot += 1\n",
    "    \n",
    "tree_batch = [mol_tree]\n",
    "\n",
    "#ret = tensorize(mol_batch,vocab,assm)\n",
    "set_batch_nodeID(tree_batch, vocab)\n",
    "smiles_batch = [tree.smiles for tree in tree_batch]\n",
    "jtenc_holder,mess_dict = JTNNEncoder.tensorize(tree_batch)\n",
    "jtenc_holder = jtenc_holder\n",
    "mpn_holder = MPN.tensorize(smiles_batch)\n",
    "\n",
    "cands = []\n",
    "batch_idx = []\n",
    "for i,mol_tree in enumerate(tree_batch):\n",
    "    for node in mol_tree.nodes:\n",
    "        print(node.smiles,node.is_leaf,len(node.cands))\n",
    "        #Leaf node's attachment is determined by neighboring node's attachment\n",
    "        if node.is_leaf or len(node.cands) == 1:\n",
    "            #print(\"test\")\n",
    "            continue\n",
    "        cands.extend( [(cand, mol_tree.nodes, node) for cand in node.cands] )\n",
    "        batch_idx.extend([i] * len(node.cands))\n",
    "print([one[0] for one in cands])\n",
    "jtmpn_holder = JTMPN.tensorize(cands, mess_dict)\n",
    "batch_idx = torch.LongTensor(batch_idx)\n",
    "print(len(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = \"O=C(C)c(c1)cccc1\"\n",
    "smiles = \"CCCCOC(=O)c(c1)c(ccc1)C(=O)OCCCC\"\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "#mol = Chem.AddHs(mol)\n",
    "print(Chem.MolToSmiles(mol))\n",
    "Draw.MolsToGridImage([mol])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtvae",
   "language": "python",
   "name": "jtvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
