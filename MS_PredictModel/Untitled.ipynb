{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training program for Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データプレプロセス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vocablary作成\n",
    "すでにvocab.txtが作成済みである場合不用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user a\n",
      " ·\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong: 1045 (28000): Access denied for user 'a'@'localhost' (using password: YES)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'connect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d12f68532f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpasswd\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connect' is not defined"
     ]
    }
   ],
   "source": [
    "from fast_jtnn.mol_tree import MolTree\n",
    "from getpass import getpass,getuser\n",
    "\n",
    "import mysql\n",
    "from mysql import connector\n",
    "import warnings\n",
    "\n",
    "# SQL server profile\n",
    "host = \"localhost\"\n",
    "user = None\n",
    "passwd = None\n",
    "port = 3306\n",
    "database=\"chemoinfo\"\n",
    "\n",
    "# \n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "# get massbank data from SQL server\n",
    "try:\n",
    "    if not isinstance(user,str):\n",
    "        user = raw_input(\"user\")\n",
    "    if not isinstance(passwd,str):\n",
    "        passwd = getpass()\n",
    "    connect = connector.connect(host=host,user=user,password=passwd,port=port,database=database)\n",
    "    cursor = connect.cursor()\n",
    "    cursor.execute(\"\"\"select smiles from massbank where ms_type=\"MS\" and instrument_type=\"EI-B\" and smiles<>'N/A'; \"\"\")\n",
    "    smiles_list = cursor.fetchall()\n",
    "except mysql.connector.Error as e:\n",
    "    print(\"Something went wrong: {}\".format(e))\n",
    "    sys.exit(1)\n",
    "finally:\n",
    "    if passwd : del passwd\n",
    "    if connect: connect.close()\n",
    "    if cursor: cursor.close()\n",
    "\n",
    "# create vocablary\n",
    "succes = 0\n",
    "fault = 0\n",
    "cset = set()\n",
    "for one in smiles_list:\n",
    "    try:\n",
    "        mol = MolTree(one[0])\n",
    "    except AttributeError as e:\n",
    "        warnings.warn(\"Entered An SMILES that does not meet the rules\")\n",
    "        continue\n",
    "    for c in mol.nodes:\n",
    "        cset.add(c.smiles)\n",
    "\n",
    "# write vocab\n",
    "with open(VOCAB_FILE,\"w\") as f:\n",
    "    for one in cset:\n",
    "        f.write(one+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! mkdir vae_model/\n",
    "%run ../fast_molvae/vae_train.py --train processed --vocab ./MS_vocab.txt --save_dir vae_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab,datasetのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of train dataset :', 6716)\n",
      "('number of validation dataset :', 747)\n"
     ]
    }
   ],
   "source": [
    "from fast_jtnn import *\n",
    "from MS_PredictModel import MS_Dataset,MS_Dataset_pickle,dataset_load\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(VOCAB_FILE,\"r\")]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "'''\n",
    "MS_Dataset.QUERY = \"\"\"select smiles,file_path from massbank where ms_type=\"MS\" and instrument_type=\"EI-B\" and smiles<>'N/A';\"\"\"\n",
    "dataset = MS_Dataset(vocab=vocab,host=\"localhost\",database=\"chemoinfo\",batch_size=20)\n",
    "'''\n",
    "train_vali_rate = 0.9\n",
    "\n",
    "train_dataset, vali_dataset = dataset_load(\"./massbank.pkl\",vocab,20,train_vali_rate)\n",
    "print(\"number of train dataset :\",len(train_dataset))\n",
    "print(\"number of validation dataset :\",len(vali_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(365, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(365, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=365, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder_lstm(\n",
      "  (embedding): Embedding(1000, 5)\n",
      "  (rnn): GRU(6, 200, batch_first=True)\n",
      "  (out): Linear(in_features=200, out_features=56, bias=True)\n",
      ")\n",
      "Model #Params: 367K\n",
      "Model #Params: 141K\n"
     ]
    }
   ],
   "source": [
    "from ms_encoder import ms_peak_encoder,ms_peak_encoder_lstm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "hidden_size = 100\n",
    "latent_size = 56\n",
    "depthT = 20\n",
    "depthG = 3\n",
    "\n",
    "dec_model = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG).to('cuda')\n",
    "print dec_model\n",
    "enc_model = ms_peak_encoder_lstm(train_dataset.max_spectrum_size,output_size=latent_size,hidden_size=200,embedding_size=5,num_rnn_layers=1,bidirectional=False).to('cuda')\n",
    "print enc_model\n",
    "\n",
    "for param in dec_model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)\n",
    "load_model = \"./vae_model/model.iter-110000\"\n",
    "dec_model.load_state_dict(torch.load(load_model,map_location='cuda'))\n",
    "\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in dec_model.parameters()]) / 1000,)\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in enc_model.parameters()]) / 1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = optim.Adam(enc_model.parameters(), lr=1e-3)\n",
    "#optimizer = optim.SGD(enc_model.parameters(),lr=5e-2,momentum=0.9)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] , Word: 46.34, Topo: 88.56, Assm: 89.66 vali_Word: 44.18, vali_Topo: 87.50, vali_assm: 89.72, learning rate: 0.0010\n",
      "[400] , Word: 47.06, Topo: 87.92, Assm: 89.85 vali_Word: 51.79, vali_Topo: 87.45, vali_assm: 91.65, learning rate: 0.0010\n",
      "[600] , Word: 47.33, Topo: 88.18, Assm: 89.43 vali_Word: 50.01, vali_Topo: 87.73, vali_assm: 89.48, learning rate: 0.0010\n",
      "('epoch : ', 1)\n",
      "[800] , Word: 47.71, Topo: 88.29, Assm: 90.67 vali_Word: 46.72, vali_Topo: 87.61, vali_assm: 89.34, learning rate: 0.0010\n",
      "[1000] , Word: 46.89, Topo: 87.98, Assm: 89.47 vali_Word: 43.46, vali_Topo: 87.50, vali_assm: 91.01, learning rate: 0.0010\n",
      "[1200] , Word: 47.97, Topo: 88.35, Assm: 89.90 vali_Word: 50.76, vali_Topo: 87.89, vali_assm: 90.66, learning rate: 0.0010\n",
      "('epoch : ', 2)\n",
      "[1400] , Word: 48.89, Topo: 88.62, Assm: 90.41 vali_Word: 48.27, vali_Topo: 88.96, vali_assm: 90.54, learning rate: 0.0010\n",
      "[1600] , Word: 48.37, Topo: 88.82, Assm: 89.49 vali_Word: 47.70, vali_Topo: 88.36, vali_assm: 90.52, learning rate: 0.0010\n",
      "[1800] , Word: 48.75, Topo: 89.43, Assm: 90.91 vali_Word: 52.13, vali_Topo: 89.16, vali_assm: 90.83, learning rate: 0.0010\n",
      "[2000] , Word: 49.66, Topo: 89.34, Assm: 89.94 vali_Word: 52.06, vali_Topo: 89.10, vali_assm: 90.03, learning rate: 0.0010\n",
      "('epoch : ', 3)\n",
      "[2200] , Word: 51.09, Topo: 89.69, Assm: 90.18 vali_Word: 53.24, vali_Topo: 89.07, vali_assm: 91.57, learning rate: 0.0010\n",
      "[2400] , Word: 50.90, Topo: 89.37, Assm: 90.79 vali_Word: 53.41, vali_Topo: 88.89, vali_assm: 90.63, learning rate: 0.0010\n",
      "[2600] , Word: 50.29, Topo: 89.67, Assm: 89.68 vali_Word: 52.93, vali_Topo: 89.66, vali_assm: 90.88, learning rate: 0.0010\n",
      "('epoch : ', 4)\n",
      "[2800] , Word: 50.97, Topo: 89.39, Assm: 90.57 vali_Word: 53.77, vali_Topo: 89.16, vali_assm: 91.10, learning rate: 0.0010\n",
      "[3000] , Word: 51.27, Topo: 89.75, Assm: 89.94 vali_Word: 54.59, vali_Topo: 89.35, vali_assm: 89.27, learning rate: 0.0010\n",
      "[3200] , Word: 51.08, Topo: 89.74, Assm: 90.81 vali_Word: 52.91, vali_Topo: 89.56, vali_assm: 91.20, learning rate: 0.0010\n",
      "('epoch : ', 5)\n",
      "[3400] , Word: 52.14, Topo: 89.98, Assm: 90.41 vali_Word: 52.78, vali_Topo: 89.78, vali_assm: 90.35, learning rate: 0.0010\n",
      "[3600] , Word: 52.46, Topo: 90.33, Assm: 90.52 vali_Word: 53.26, vali_Topo: 90.14, vali_assm: 90.85, learning rate: 0.0010\n",
      "[3800] , Word: 52.12, Topo: 90.48, Assm: 91.01 vali_Word: 52.23, vali_Topo: 90.05, vali_assm: 89.63, learning rate: 0.0010\n",
      "[4000] , Word: 52.32, Topo: 90.34, Assm: 90.25 vali_Word: 50.82, vali_Topo: 90.14, vali_assm: 90.96, learning rate: 0.0010\n",
      "('epoch : ', 6)\n",
      "[4200] , Word: 52.58, Topo: 90.59, Assm: 90.97 vali_Word: 54.95, vali_Topo: 90.46, vali_assm: 90.95, learning rate: 0.0010\n",
      "[4400] , Word: 53.05, Topo: 90.86, Assm: 89.68 vali_Word: 52.39, vali_Topo: 90.37, vali_assm: 90.47, learning rate: 0.0010\n",
      "[4600] , Word: 54.43, Topo: 90.90, Assm: 91.22 vali_Word: 56.40, vali_Topo: 90.78, vali_assm: 90.57, learning rate: 0.0010\n",
      "('epoch : ', 7)\n",
      "[4800] , Word: 53.64, Topo: 90.98, Assm: 90.22 vali_Word: 51.46, vali_Topo: 90.78, vali_assm: 91.28, learning rate: 0.0010\n",
      "[5000] , Word: 54.46, Topo: 91.00, Assm: 90.52 vali_Word: 53.94, vali_Topo: 90.27, vali_assm: 90.57, learning rate: 0.0010\n",
      "[5200] , Word: 54.52, Topo: 90.88, Assm: 91.05 vali_Word: 54.94, vali_Topo: 91.01, vali_assm: 90.15, learning rate: 0.0010\n",
      "('epoch : ', 8)\n",
      "[5400] , Word: 54.57, Topo: 91.14, Assm: 89.68 vali_Word: 55.15, vali_Topo: 91.08, vali_assm: 90.24, learning rate: 0.0010\n",
      "[5600] , Word: 55.49, Topo: 91.30, Assm: 91.91 vali_Word: 47.44, vali_Topo: 91.07, vali_assm: 90.66, learning rate: 0.0010\n",
      "[5800] , Word: 54.64, Topo: 91.23, Assm: 88.83 vali_Word: 55.50, vali_Topo: 90.87, vali_assm: 90.71, learning rate: 0.0010\n",
      "[6000] , Word: 55.13, Topo: 91.52, Assm: 90.14 vali_Word: 55.93, vali_Topo: 91.15, vali_assm: 89.45, learning rate: 0.0010\n",
      "('epoch : ', 9)\n",
      "[6200] , Word: 55.34, Topo: 91.35, Assm: 90.65 vali_Word: 56.53, vali_Topo: 91.43, vali_assm: 90.60, learning rate: 0.0010\n",
      "[6400] , Word: 55.29, Topo: 91.66, Assm: 90.45 vali_Word: 54.07, vali_Topo: 90.99, vali_assm: 90.51, learning rate: 0.0010\n",
      "[6600] , Word: 56.02, Topo: 91.65, Assm: 91.31 vali_Word: 53.83, vali_Topo: 91.09, vali_assm: 90.17, learning rate: 0.0010\n",
      "('epoch : ', 10)\n",
      "[6800] , Word: 56.48, Topo: 91.94, Assm: 90.63 vali_Word: 54.19, vali_Topo: 91.00, vali_assm: 91.02, learning rate: 0.0010\n",
      "[7000] , Word: 55.49, Topo: 91.55, Assm: 90.49 vali_Word: 55.32, vali_Topo: 91.29, vali_assm: 90.63, learning rate: 0.0010\n",
      "[7200] , Word: 56.26, Topo: 91.88, Assm: 89.54 vali_Word: 55.92, vali_Topo: 90.82, vali_assm: 90.40, learning rate: 0.0010\n",
      "('epoch : ', 11)\n",
      "[7400] , Word: 56.57, Topo: 91.93, Assm: 91.13 vali_Word: 55.54, vali_Topo: 91.31, vali_assm: 91.17, learning rate: 0.0010\n",
      "[7600] , Word: 56.67, Topo: 92.18, Assm: 90.81 vali_Word: 57.06, vali_Topo: 91.27, vali_assm: 90.14, learning rate: 0.0009\n",
      "[7800] , Word: 57.36, Topo: 92.05, Assm: 91.34 vali_Word: 56.19, vali_Topo: 91.41, vali_assm: 90.63, learning rate: 0.0009\n",
      "[8000] , Word: 57.40, Topo: 92.02, Assm: 90.93 vali_Word: 56.78, vali_Topo: 91.29, vali_assm: 90.30, learning rate: 0.0009\n",
      "('epoch : ', 12)\n",
      "[8200] , Word: 58.43, Topo: 92.19, Assm: 91.25 vali_Word: 56.02, vali_Topo: 91.74, vali_assm: 90.07, learning rate: 0.0009\n",
      "[8400] , Word: 57.11, Topo: 92.26, Assm: 90.14 vali_Word: 54.88, vali_Topo: 90.77, vali_assm: 90.92, learning rate: 0.0009\n",
      "[8600] , Word: 56.96, Topo: 92.29, Assm: 90.34 vali_Word: 56.24, vali_Topo: 91.67, vali_assm: 90.71, learning rate: 0.0009\n",
      "('epoch : ', 13)\n",
      "[8800] , Word: 58.59, Topo: 92.40, Assm: 90.98 vali_Word: 58.15, vali_Topo: 91.59, vali_assm: 89.60, learning rate: 0.0009\n",
      "[9000] , Word: 58.30, Topo: 92.81, Assm: 91.02 vali_Word: 57.07, vali_Topo: 91.79, vali_assm: 90.93, learning rate: 0.0009\n",
      "[9200] , Word: 58.37, Topo: 92.51, Assm: 91.23 vali_Word: 56.13, vali_Topo: 91.84, vali_assm: 91.32, learning rate: 0.0009\n",
      "('epoch : ', 14)\n",
      "[9400] , Word: 58.50, Topo: 92.44, Assm: 91.04 vali_Word: 58.46, vali_Topo: 91.68, vali_assm: 90.83, learning rate: 0.0009\n",
      "[9600] , Word: 58.91, Topo: 92.89, Assm: 91.08 vali_Word: 56.78, vali_Topo: 92.26, vali_assm: 89.91, learning rate: 0.0009\n",
      "[9800] , Word: 58.71, Topo: 92.64, Assm: 90.98 vali_Word: 57.39, vali_Topo: 91.90, vali_assm: 88.15, learning rate: 0.0009\n",
      "[10000] , Word: 60.07, Topo: 93.20, Assm: 91.22 vali_Word: 58.14, vali_Topo: 91.86, vali_assm: 90.09, learning rate: 0.0009\n",
      "('epoch : ', 15)\n",
      "[10200] , Word: 59.95, Topo: 93.20, Assm: 91.11 vali_Word: 56.11, vali_Topo: 91.99, vali_assm: 89.79, learning rate: 0.0009\n",
      "[10400] , Word: 59.33, Topo: 93.00, Assm: 91.35 vali_Word: 59.02, vali_Topo: 92.23, vali_assm: 89.92, learning rate: 0.0009\n",
      "[10600] , Word: 60.58, Topo: 93.34, Assm: 91.13 vali_Word: 58.35, vali_Topo: 92.10, vali_assm: 89.82, learning rate: 0.0009\n",
      "('epoch : ', 16)\n",
      "[10800] , Word: 61.38, Topo: 93.28, Assm: 90.65 vali_Word: 58.37, vali_Topo: 92.42, vali_assm: 91.00, learning rate: 0.0009\n",
      "[11000] , Word: 61.25, Topo: 93.39, Assm: 90.94 vali_Word: 57.77, vali_Topo: 92.25, vali_assm: 90.58, learning rate: 0.0009\n",
      "[11200] , Word: 60.86, Topo: 93.51, Assm: 90.98 vali_Word: 56.27, vali_Topo: 92.43, vali_assm: 89.56, learning rate: 0.0009\n",
      "[11400] , Word: 60.87, Topo: 93.58, Assm: 91.67 vali_Word: 57.78, vali_Topo: 92.25, vali_assm: 90.72, learning rate: 0.0009\n",
      "('epoch : ', 17)\n",
      "[11600] , Word: 62.92, Topo: 93.79, Assm: 91.56 vali_Word: 58.81, vali_Topo: 92.37, vali_assm: 91.67, learning rate: 0.0009\n",
      "[11800] , Word: 61.48, Topo: 93.86, Assm: 91.49 vali_Word: 58.58, vali_Topo: 92.58, vali_assm: 90.23, learning rate: 0.0009\n",
      "[12000] , Word: 61.79, Topo: 93.63, Assm: 90.95 vali_Word: 57.81, vali_Topo: 92.53, vali_assm: 90.85, learning rate: 0.0009\n",
      "('epoch : ', 18)\n",
      "[12200] , Word: 63.14, Topo: 94.24, Assm: 91.64 vali_Word: 57.05, vali_Topo: 92.49, vali_assm: 90.17, learning rate: 0.0009\n",
      "[12400] , Word: 62.99, Topo: 94.02, Assm: 92.17 vali_Word: 57.50, vali_Topo: 92.68, vali_assm: 90.23, learning rate: 0.0009\n",
      "[12600] , Word: 63.06, Topo: 94.25, Assm: 91.76 vali_Word: 57.92, vali_Topo: 92.91, vali_assm: 89.53, learning rate: 0.0009\n",
      "('epoch : ', 19)\n",
      "[12800] , Word: 63.85, Topo: 94.03, Assm: 92.03 vali_Word: 56.54, vali_Topo: 92.87, vali_assm: 91.11, learning rate: 0.0009\n",
      "[13000] , Word: 63.36, Topo: 94.20, Assm: 91.47 vali_Word: 57.27, vali_Topo: 92.99, vali_assm: 91.24, learning rate: 0.0009\n",
      "[13200] , Word: 64.68, Topo: 94.46, Assm: 91.73 vali_Word: 56.15, vali_Topo: 92.57, vali_assm: 90.20, learning rate: 0.0009\n",
      "[13400] , Word: 63.87, Topo: 94.24, Assm: 91.84 vali_Word: 56.51, vali_Topo: 92.57, vali_assm: 90.30, learning rate: 0.0009\n",
      "('epoch : ', 20)\n",
      "[13600] , Word: 65.50, Topo: 94.61, Assm: 91.54 vali_Word: 57.90, vali_Topo: 92.82, vali_assm: 90.87, learning rate: 0.0009\n",
      "[13800] , Word: 64.96, Topo: 94.68, Assm: 91.67 vali_Word: 57.39, vali_Topo: 92.48, vali_assm: 91.28, learning rate: 0.0009\n",
      "[14000] , Word: 64.28, Topo: 94.63, Assm: 91.76 vali_Word: 57.58, vali_Topo: 92.83, vali_assm: 89.57, learning rate: 0.0009\n",
      "('epoch : ', 21)\n",
      "[14200] , Word: 65.32, Topo: 94.69, Assm: 91.92 vali_Word: 57.28, vali_Topo: 92.92, vali_assm: 90.74, learning rate: 0.0009\n",
      "[14400] , Word: 65.83, Topo: 94.77, Assm: 90.44 vali_Word: 57.50, vali_Topo: 92.88, vali_assm: 90.18, learning rate: 0.0009\n",
      "[14600] , Word: 66.23, Topo: 95.08, Assm: 91.80 vali_Word: 58.02, vali_Topo: 93.00, vali_assm: 91.15, learning rate: 0.0009\n",
      "('epoch : ', 22)\n",
      "[14800] , Word: 66.47, Topo: 94.93, Assm: 92.21 vali_Word: 58.32, vali_Topo: 93.03, vali_assm: 90.26, learning rate: 0.0009\n",
      "[15000] , Word: 67.11, Topo: 95.30, Assm: 92.08 vali_Word: 57.31, vali_Topo: 92.71, vali_assm: 90.80, learning rate: 0.0008\n",
      "[15200] , Word: 67.05, Topo: 95.09, Assm: 91.55 vali_Word: 57.94, vali_Topo: 92.77, vali_assm: 90.40, learning rate: 0.0008\n",
      "[15400] , Word: 67.06, Topo: 94.83, Assm: 91.52 vali_Word: 57.40, vali_Topo: 92.92, vali_assm: 90.93, learning rate: 0.0008\n",
      "('epoch : ', 23)\n",
      "[15600] , Word: 68.10, Topo: 95.24, Assm: 92.39 vali_Word: 56.81, vali_Topo: 92.88, vali_assm: 88.67, learning rate: 0.0008\n",
      "[15800] , Word: 67.94, Topo: 95.56, Assm: 91.68 vali_Word: 58.23, vali_Topo: 93.02, vali_assm: 90.86, learning rate: 0.0008\n",
      "[16000] , Word: 68.53, Topo: 95.46, Assm: 91.87 vali_Word: 58.63, vali_Topo: 92.93, vali_assm: 90.62, learning rate: 0.0008\n",
      "('epoch : ', 24)\n",
      "[16200] , Word: 69.07, Topo: 95.36, Assm: 91.42 vali_Word: 56.58, vali_Topo: 92.87, vali_assm: 90.49, learning rate: 0.0008\n",
      "[16400] , Word: 68.86, Topo: 95.48, Assm: 91.62 vali_Word: 57.39, vali_Topo: 92.83, vali_assm: 89.93, learning rate: 0.0008\n",
      "[16600] , Word: 68.15, Topo: 95.52, Assm: 91.40 vali_Word: 57.75, vali_Topo: 93.09, vali_assm: 90.26, learning rate: 0.0008\n",
      "('epoch : ', 25)\n",
      "[16800] , Word: 69.69, Topo: 95.29, Assm: 92.50 vali_Word: 56.38, vali_Topo: 92.96, vali_assm: 90.72, learning rate: 0.0008\n",
      "[17000] , Word: 69.90, Topo: 95.79, Assm: 92.51 vali_Word: 59.12, vali_Topo: 92.94, vali_assm: 91.65, learning rate: 0.0008\n",
      "[17200] , Word: 69.86, Topo: 95.75, Assm: 92.50 vali_Word: 57.07, vali_Topo: 92.97, vali_assm: 90.00, learning rate: 0.0008\n",
      "[17400] , Word: 69.39, Topo: 95.80, Assm: 91.81 vali_Word: 58.99, vali_Topo: 92.79, vali_assm: 90.96, learning rate: 0.0008\n",
      "('epoch : ', 26)\n",
      "[17600] , Word: 70.91, Topo: 95.93, Assm: 92.48 vali_Word: 57.45, vali_Topo: 92.85, vali_assm: 90.22, learning rate: 0.0008\n",
      "[17800] , Word: 71.26, Topo: 96.06, Assm: 91.94 vali_Word: 56.92, vali_Topo: 92.62, vali_assm: 90.10, learning rate: 0.0008\n",
      "[18000] , Word: 70.27, Topo: 95.60, Assm: 91.37 vali_Word: 57.33, vali_Topo: 92.76, vali_assm: 91.26, learning rate: 0.0008\n",
      "('epoch : ', 27)\n",
      "[18200] , Word: 70.60, Topo: 95.82, Assm: 91.13 vali_Word: 57.47, vali_Topo: 92.96, vali_assm: 91.57, learning rate: 0.0008\n",
      "[18400] , Word: 70.82, Topo: 95.95, Assm: 91.92 vali_Word: 57.33, vali_Topo: 92.77, vali_assm: 90.75, learning rate: 0.0008\n",
      "[18600] , Word: 71.29, Topo: 95.98, Assm: 92.50 vali_Word: 57.88, vali_Topo: 92.92, vali_assm: 90.64, learning rate: 0.0008\n",
      "('epoch : ', 28)\n",
      "[18800] , Word: 70.97, Topo: 95.91, Assm: 92.09 vali_Word: 57.23, vali_Topo: 92.63, vali_assm: 89.97, learning rate: 0.0008\n",
      "[19000] , Word: 71.70, Topo: 96.15, Assm: 91.94 vali_Word: 56.70, vali_Topo: 92.82, vali_assm: 90.96, learning rate: 0.0008\n",
      "[19200] , Word: 71.39, Topo: 96.10, Assm: 91.84 vali_Word: 57.76, vali_Topo: 92.86, vali_assm: 90.17, learning rate: 0.0008\n",
      "[19400] , Word: 71.50, Topo: 96.11, Assm: 91.88 vali_Word: 57.75, vali_Topo: 92.70, vali_assm: 89.36, learning rate: 0.0008\n",
      "('epoch : ', 29)\n",
      "[19600] , Word: 72.46, Topo: 96.29, Assm: 92.67 vali_Word: 56.46, vali_Topo: 92.76, vali_assm: 90.06, learning rate: 0.0008\n",
      "[19800] , Word: 73.50, Topo: 96.26, Assm: 92.30 vali_Word: 56.74, vali_Topo: 92.70, vali_assm: 90.55, learning rate: 0.0008\n",
      "[20000] , Word: 72.17, Topo: 96.32, Assm: 92.16 vali_Word: 57.80, vali_Topo: 92.64, vali_assm: 90.41, learning rate: 0.0008\n",
      "('epoch : ', 30)\n",
      "[20200] , Word: 71.96, Topo: 96.33, Assm: 92.19 vali_Word: 57.75, vali_Topo: 92.98, vali_assm: 89.48, learning rate: 0.0008\n",
      "[20400] , Word: 73.38, Topo: 96.28, Assm: 91.94 vali_Word: 57.21, vali_Topo: 92.89, vali_assm: 90.03, learning rate: 0.0008\n",
      "[20600] , Word: 72.99, Topo: 96.52, Assm: 91.78 vali_Word: 55.59, vali_Topo: 92.81, vali_assm: 90.30, learning rate: 0.0008\n",
      "[20800] , Word: 72.70, Topo: 96.13, Assm: 92.16 vali_Word: 57.16, vali_Topo: 92.76, vali_assm: 90.57, learning rate: 0.0008\n",
      "('epoch : ', 31)\n",
      "[21000] , Word: 73.39, Topo: 96.50, Assm: 92.56 vali_Word: 56.06, vali_Topo: 92.98, vali_assm: 90.70, learning rate: 0.0008\n",
      "[21200] , Word: 73.11, Topo: 96.49, Assm: 92.20 vali_Word: 57.20, vali_Topo: 92.80, vali_assm: 90.45, learning rate: 0.0008\n",
      "[21400] , Word: 73.94, Topo: 96.33, Assm: 92.17 vali_Word: 57.11, vali_Topo: 92.49, vali_assm: 89.78, learning rate: 0.0008\n",
      "('epoch : ', 32)\n",
      "[21600] , Word: 75.06, Topo: 96.74, Assm: 92.83 vali_Word: 57.25, vali_Topo: 92.61, vali_assm: 89.04, learning rate: 0.0008\n",
      "[21800] , Word: 74.25, Topo: 96.61, Assm: 92.06 vali_Word: 55.20, vali_Topo: 92.64, vali_assm: 89.28, learning rate: 0.0008\n",
      "[22000] , Word: 73.76, Topo: 96.49, Assm: 92.67 vali_Word: 57.19, vali_Topo: 92.71, vali_assm: 90.77, learning rate: 0.0008\n",
      "('epoch : ', 33)\n",
      "[22200] , Word: 73.91, Topo: 96.56, Assm: 93.39 vali_Word: 57.44, vali_Topo: 92.59, vali_assm: 90.65, learning rate: 0.0008\n",
      "[22400] , Word: 75.13, Topo: 96.71, Assm: 92.99 vali_Word: 56.41, vali_Topo: 92.77, vali_assm: 90.46, learning rate: 0.0007\n",
      "[22600] , Word: 75.13, Topo: 96.74, Assm: 93.36 vali_Word: 54.80, vali_Topo: 92.77, vali_assm: 90.59, learning rate: 0.0007\n",
      "[22800] , Word: 75.16, Topo: 96.53, Assm: 92.00 vali_Word: 56.71, vali_Topo: 92.45, vali_assm: 90.27, learning rate: 0.0007\n",
      "('epoch : ', 34)\n",
      "[23000] , Word: 76.03, Topo: 96.85, Assm: 92.76 vali_Word: 56.53, vali_Topo: 92.66, vali_assm: 90.42, learning rate: 0.0007\n",
      "[23200] , Word: 75.86, Topo: 96.82, Assm: 93.18 vali_Word: 55.83, vali_Topo: 92.55, vali_assm: 90.87, learning rate: 0.0007\n",
      "[23400] , Word: 75.83, Topo: 96.97, Assm: 92.45 vali_Word: 56.11, vali_Topo: 92.53, vali_assm: 89.69, learning rate: 0.0007\n",
      "('epoch : ', 35)\n",
      "[23600] , Word: 76.73, Topo: 96.99, Assm: 92.58 vali_Word: 57.24, vali_Topo: 92.74, vali_assm: 90.43, learning rate: 0.0007\n",
      "[23800] , Word: 77.00, Topo: 96.91, Assm: 91.71 vali_Word: 56.27, vali_Topo: 92.50, vali_assm: 91.03, learning rate: 0.0007\n",
      "[24000] , Word: 75.81, Topo: 96.90, Assm: 92.50 vali_Word: 57.79, vali_Topo: 92.50, vali_assm: 90.17, learning rate: 0.0007\n",
      "('epoch : ', 36)\n",
      "[24200] , Word: 76.33, Topo: 96.89, Assm: 92.75 vali_Word: 56.49, vali_Topo: 92.56, vali_assm: 89.50, learning rate: 0.0007\n",
      "[24400] , Word: 77.18, Topo: 97.22, Assm: 92.33 vali_Word: 55.79, vali_Topo: 92.48, vali_assm: 89.77, learning rate: 0.0007\n",
      "[24600] , Word: 76.70, Topo: 97.02, Assm: 92.50 vali_Word: 56.39, vali_Topo: 92.74, vali_assm: 89.55, learning rate: 0.0007\n",
      "[24800] , Word: 76.61, Topo: 97.01, Assm: 93.06 vali_Word: 57.18, vali_Topo: 92.71, vali_assm: 91.02, learning rate: 0.0007\n",
      "('epoch : ', 37)\n",
      "[25000] , Word: 77.41, Topo: 97.09, Assm: 92.38 vali_Word: 55.73, vali_Topo: 92.99, vali_assm: 90.01, learning rate: 0.0007\n",
      "[25200] , Word: 77.58, Topo: 97.21, Assm: 93.45 vali_Word: 56.28, vali_Topo: 92.69, vali_assm: 89.69, learning rate: 0.0007\n",
      "[25400] , Word: 77.09, Topo: 97.07, Assm: 92.59 vali_Word: 56.86, vali_Topo: 92.54, vali_assm: 89.94, learning rate: 0.0007\n",
      "('epoch : ', 38)\n",
      "[25600] , Word: 77.38, Topo: 97.22, Assm: 92.91 vali_Word: 56.04, vali_Topo: 92.72, vali_assm: 90.29, learning rate: 0.0007\n",
      "[25800] , Word: 78.11, Topo: 97.28, Assm: 93.10 vali_Word: 56.32, vali_Topo: 92.54, vali_assm: 90.94, learning rate: 0.0007\n",
      "[26000] , Word: 77.77, Topo: 97.14, Assm: 92.06 vali_Word: 56.66, vali_Topo: 92.55, vali_assm: 90.28, learning rate: 0.0007\n",
      "('epoch : ', 39)\n",
      "[26200] , Word: 77.54, Topo: 97.25, Assm: 93.87 vali_Word: 55.81, vali_Topo: 92.79, vali_assm: 89.31, learning rate: 0.0007\n",
      "[26400] , Word: 78.31, Topo: 97.18, Assm: 92.57 vali_Word: 56.23, vali_Topo: 92.66, vali_assm: 89.37, learning rate: 0.0007\n",
      "[26600] , Word: 78.14, Topo: 97.26, Assm: 93.58 vali_Word: 56.77, vali_Topo: 92.72, vali_assm: 89.73, learning rate: 0.0007\n",
      "[26800] , Word: 77.97, Topo: 97.21, Assm: 93.29 vali_Word: 56.24, vali_Topo: 92.55, vali_assm: 89.66, learning rate: 0.0007\n",
      "('epoch : ', 40)\n",
      "[27000] , Word: 78.04, Topo: 97.31, Assm: 92.88 vali_Word: 54.67, vali_Topo: 92.42, vali_assm: 89.73, learning rate: 0.0007\n",
      "[27200] , Word: 79.18, Topo: 97.36, Assm: 93.45 vali_Word: 56.23, vali_Topo: 92.84, vali_assm: 90.12, learning rate: 0.0007\n",
      "[27400] , Word: 78.31, Topo: 97.09, Assm: 92.72 vali_Word: 55.37, vali_Topo: 92.59, vali_assm: 90.24, learning rate: 0.0007\n",
      "('epoch : ', 41)\n",
      "[27600] , Word: 78.07, Topo: 97.23, Assm: 92.64 vali_Word: 55.70, vali_Topo: 92.54, vali_assm: 90.67, learning rate: 0.0007\n",
      "[27800] , Word: 79.27, Topo: 97.35, Assm: 92.91 vali_Word: 55.03, vali_Topo: 92.85, vali_assm: 90.00, learning rate: 0.0007\n",
      "[28000] , Word: 78.67, Topo: 97.53, Assm: 94.09 vali_Word: 56.75, vali_Topo: 92.82, vali_assm: 89.11, learning rate: 0.0007\n",
      "('epoch : ', 42)\n",
      "[28200] , Word: 78.32, Topo: 97.40, Assm: 92.86 vali_Word: 55.42, vali_Topo: 92.56, vali_assm: 89.28, learning rate: 0.0007\n",
      "[28400] , Word: 80.21, Topo: 97.39, Assm: 93.25 vali_Word: 55.62, vali_Topo: 92.39, vali_assm: 90.21, learning rate: 0.0007\n",
      "[28600] , Word: 79.30, Topo: 97.41, Assm: 92.65 vali_Word: 54.93, vali_Topo: 92.62, vali_assm: 89.21, learning rate: 0.0007\n",
      "[28800] , Word: 78.93, Topo: 97.38, Assm: 93.42 vali_Word: 55.72, vali_Topo: 92.48, vali_assm: 88.76, learning rate: 0.0007\n",
      "('epoch : ', 43)\n",
      "[29000] , Word: 79.59, Topo: 97.49, Assm: 93.81 vali_Word: 56.07, vali_Topo: 92.47, vali_assm: 89.71, learning rate: 0.0007\n",
      "[29200] , Word: 78.73, Topo: 97.22, Assm: 93.22 vali_Word: 56.47, vali_Topo: 92.59, vali_assm: 90.45, learning rate: 0.0007\n",
      "[29400] , Word: 78.69, Topo: 97.47, Assm: 92.70 vali_Word: 56.09, vali_Topo: 92.67, vali_assm: 89.96, learning rate: 0.0007\n",
      "('epoch : ', 44)\n",
      "[29600] , Word: 79.24, Topo: 97.28, Assm: 92.52 vali_Word: 55.83, vali_Topo: 92.54, vali_assm: 89.52, learning rate: 0.0007\n",
      "[29800] , Word: 80.83, Topo: 97.57, Assm: 93.47 vali_Word: 55.61, vali_Topo: 92.33, vali_assm: 89.04, learning rate: 0.0007\n",
      "[30000] , Word: 80.36, Topo: 97.67, Assm: 93.38 vali_Word: 55.00, vali_Topo: 92.40, vali_assm: 89.93, learning rate: 0.0007\n",
      "('epoch : ', 45)\n",
      "[30200] , Word: 80.27, Topo: 97.67, Assm: 93.46 vali_Word: 55.74, vali_Topo: 92.64, vali_assm: 89.66, learning rate: 0.0007\n",
      "[30400] , Word: 81.64, Topo: 97.77, Assm: 94.29 vali_Word: 54.84, vali_Topo: 92.54, vali_assm: 90.18, learning rate: 0.0007\n",
      "[30600] , Word: 81.03, Topo: 97.78, Assm: 93.15 vali_Word: 54.89, vali_Topo: 92.70, vali_assm: 90.17, learning rate: 0.0007\n",
      "[30800] , Word: 80.98, Topo: 97.66, Assm: 93.35 vali_Word: 55.33, vali_Topo: 92.42, vali_assm: 89.67, learning rate: 0.0007\n",
      "('epoch : ', 46)\n",
      "[31000] , Word: 81.62, Topo: 97.90, Assm: 94.16 vali_Word: 55.14, vali_Topo: 92.30, vali_assm: 90.03, learning rate: 0.0007\n",
      "[31200] , Word: 81.80, Topo: 97.83, Assm: 93.64 vali_Word: 55.90, vali_Topo: 92.23, vali_assm: 89.85, learning rate: 0.0007\n",
      "[31400] , Word: 81.35, Topo: 97.66, Assm: 93.05 vali_Word: 55.88, vali_Topo: 92.52, vali_assm: 90.42, learning rate: 0.0007\n",
      "('epoch : ', 47)\n",
      "[31600] , Word: 81.83, Topo: 97.83, Assm: 93.86 vali_Word: 55.92, vali_Topo: 92.68, vali_assm: 90.11, learning rate: 0.0007\n",
      "[31800] , Word: 81.55, Topo: 97.76, Assm: 94.27 vali_Word: 54.94, vali_Topo: 92.69, vali_assm: 89.77, learning rate: 0.0007\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-66fb07b71ae8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m#if pbar is None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#pbar = tqdm()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#if pbar is not None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-66fb07b71ae8>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(max_epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mdec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1002\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0;34mr\"\"\"Sets gradients of all model parameters to zero.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda-aisiars/envs/jtvae/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from MS_PredictModel import ms_peak_encoder,MS_Dataset\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "pbar = None\n",
    "train_dataset.batch_size = 10\n",
    "vali_dataset.batch_size = 10\n",
    "\n",
    "anneal_iter = 7400\n",
    "\n",
    "def training(max_epoch = 100):\n",
    "    global pbar\n",
    "    total_step = 0\n",
    "    meters = np.zeros(3)\n",
    "    vali_meters = np.zeros(3)\n",
    "    with open(\"log2.csv\",\"w\") as f:\n",
    "        f.write(\"epoch,iter.,word,topo,assm,vali word,vali topo,vali assm\\n\")\n",
    "    for epoch in range(max_epoch):\n",
    "        print(\"epoch : \",epoch)\n",
    "        for batch in train_dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            total_step+=1\n",
    "            #pbar.update(1)\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            \n",
    "            enc_model.zero_grad()\n",
    "            dec_model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            h = enc_model(x,y)\n",
    "            tree_vec = h[:,:h.shape[1]/2]\n",
    "            mol_vec  = h[:,h.shape[1]/2:]\n",
    "            _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "            word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "            assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "            total_loss = word_loss+topo_loss+assm_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            meters = meters + np.array([word_acc * 100, topo_acc * 100, assm_acc * 100])\n",
    "            if total_step % 200 == 0:\n",
    "                vali_total = 0\n",
    "                for batch in vali_dataset:\n",
    "                    x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "                    x = x.to('cuda')\n",
    "                    y = y.to('cuda')\n",
    "                    with torch.no_grad():\n",
    "                        h = enc_model(x,y,False)\n",
    "                        tree_vec = h[:,:h.shape[1]/2]\n",
    "                        mol_vec  = h[:,h.shape[1]/2:]\n",
    "                        _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "                        word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "                        assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "                        vali_meters = vali_meters + np.array([word_acc * 100, topo_acc * 100, assm_acc * 100])\n",
    "                        vali_total += 1\n",
    "                meters /= 200\n",
    "                vali_meters /= vali_total\n",
    "                print \"[%d] , Word: %.2f, Topo: %.2f, Assm: %.2f vali_Word: %.2f, vali_Topo: %.2f, vali_assm: %.2f, learning rate: %.4f\" % \\\n",
    "                    (total_step, meters[0], meters[1], meters[2],vali_meters[0],vali_meters[1],vali_meters[2],scheduler.get_lr()[0])\n",
    "                with open(\"log2.csv\",\"a\") as f:\n",
    "                    f.write(\"%d,%d,%.2f,%.2f,%.2f,%.2f,%.2f,%.2f\\n\" % (epoch,total_step,meters[0], meters[1], meters[2],vali_meters[0],vali_meters[1],vali_meters[2]))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "                vali_meters *= 0\n",
    "            if total_step % 200 == 0:\n",
    "                torch.save(enc_model.state_dict(), \"./enc_model\" + \"/model.iter-\" + str(total_step))\n",
    "            if total_step % anneal_iter == 0:\n",
    "                scheduler.step()\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "try:\n",
    "    #if pbar is None:\n",
    "        #pbar = tqdm()\n",
    "    training(100)\n",
    "except RuntimeError as e:\n",
    "    #if pbar is not None:\n",
    "        #del pbar\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    #import pdb; pdb.set_trace()\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    with torch.no_grad():\n",
    "        with open(\"evaluation.csv\",\"w\") as f:\n",
    "            f.write(\"true,predict\\n\")\n",
    "        for batch in vali_dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            \n",
    "            h = enc_model(x,y)\n",
    "            tree_vec = h[:,:h.shape[1]/2]\n",
    "            mol_vec  = h[:,h.shape[1]/2:]\n",
    "            for num in range(h.size()[0]):\n",
    "                with open(\"evaluation.csv\",\"a\") as f:\n",
    "                    f.write(x_batch[num].smiles+\",\"+dec_model.decode(tree_vec[num].view(1,latent_size/2),mol_vec[num].view(1,latent_size/2),False)+\"\\n\")\n",
    "            \n",
    "\n",
    "evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 740\n",
      "Number of matching: 23\n",
      "[[11, 'CCCCCCCCCCCCC(CC(O)=O)NCc(c1)cccc1'], [13, 'O=C(CCCCC(=O)O[Si](C)(C)C)O[Si](C)(C)C'], [36, 'OCc(c1)cccc1'], [54, 'CCCCCCCC=CCC[Si](C)(C)C'], [67, 'CI'], [68, 'CCOCCOCC'], [79, 'CC(C)OC(C)C'], [113, 'COc(c1)ccc(OC)c1'], [192, 'CCCCCCCC(=O)OCCCC'], [258, 'CCOc(c1)c(ccc1)C(=O)O[Si](C)(C)C'], [306, 'C(C1)CCC(C1)C(=O)O[Si](C)(C)C'], [338, 'CC(C)COC(=O)c(c1)cccc1'], [376, 'C[Si](C)(C)OC(=O)c(c1)c(O[Si](C)(C)C)cc(c2)c(ccc2)1'], [381, 'ClC(Cl)C(Cl)Cl'], [443, 'CCCCCCCCO'], [515, 'Cc(c1)ccc(c1)CC(=O)O[Si](C)(C)C'], [603, 'c(c2)ccc(c2)C(C(C1)CCC1)C(=O)O[Si](C)(C)C'], [621, 'CCCCCCC(=O)CC'], [639, 'CCCCCCCCCCCCC(=O)O[Si](C)(C)C'], [661, 'OCC(O)Cn(c2)c(C(=O)1)c(n2)N(C)C(=O)N(C)1'], [699, 'CCCCOc(c1)ccc(c1)C(=O)O[Si](C)(C)C'], [707, 'O=C(CC(C1)CCCC1)O[Si](C)(C)C'], [713, 'O=C(C1)C(C)C(=O)C1']]\n",
      "Number of matching: 35\n",
      "[[11, 'CCCCCCCCCCCCC(CC(O)=O)NCc(c1)cccc1'], [13, 'O=C(CCCCC(=O)O[Si](C)(C)C)O[Si](C)(C)C'], [36, 'OCc(c1)cccc1'], [41, 'CC(C)(C)c(c1)cccc1'], [54, 'CCCCCCCC=CCC[Si](C)(C)C'], [67, 'CI'], [68, 'CCOCCOCC'], [79, 'CC(C)OC(C)C'], [113, 'COc(c1)ccc(OC)c1'], [192, 'CCCCCCCC(=O)OCCCC'], [256, 'Oc(c1)cc(C)c(C)c1'], [258, 'CCOc(c1)c(ccc1)C(=O)O[Si](C)(C)C'], [280, 'CCCCCCCC(=O)OCCC(C)C'], [306, 'C(C1)CCC(C1)C(=O)O[Si](C)(C)C'], [338, 'CC(C)COC(=O)c(c1)cccc1'], [365, 'CCCCN'], [374, 'CCCCCCCCOC(=O)CC'], [376, 'C[Si](C)(C)OC(=O)c(c1)c(O[Si](C)(C)C)cc(c2)c(ccc2)1'], [377, 'CCCCCCCCCCCC(=O)OCCCC'], [381, 'ClC(Cl)C(Cl)Cl'], [413, 'Nc(c1)cc(C)c(C)c1'], [443, 'CCCCCCCCO'], [463, 'CCCCCCCCC(=O)OCCCC'], [515, 'Cc(c1)ccc(c1)CC(=O)O[Si](C)(C)C'], [603, 'c(c2)ccc(c2)C(C(C1)CCC1)C(=O)O[Si](C)(C)C'], [621, 'CCCCCCC(=O)CC'], [639, 'CCCCCCCCCCCCC(=O)O[Si](C)(C)C'], [661, 'OCC(O)Cn(c2)c(C(=O)1)c(n2)N(C)C(=O)N(C)1'], [665, 'CCCCCCCCCC(C)C(OC)OC'], [669, 'Cc(c1)ccc(C)c1'], [699, 'CCCCOc(c1)ccc(c1)C(=O)O[Si](C)(C)C'], [707, 'O=C(CC(C1)CCCC1)O[Si](C)(C)C'], [713, 'O=C(C1)C(C)C(=O)C1'], [729, 'CCC(C)=CCCC(C)=O'], [739, 'CC(C)c(c1)c(O)ccc1']]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "def _re_smiles(smiles1,smiles2):\n",
    "    #print(smiles1,smiles2)\n",
    "    smiles1 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles1),True)\n",
    "    smiles2 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles2),True)\n",
    "    return smiles1 == smiles2\n",
    "\n",
    "def is_structural_isomer(smiles1,smiles2):\n",
    "    def Molecular_formula(smiles):\n",
    "        atoms = {}\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        for atom in mol.GetAtoms():\n",
    "            if not atom.GetSymbol() in atoms:\n",
    "                atoms[atom.GetSymbol()] = 1\n",
    "            else:\n",
    "                atoms[atom.GetSymbol()] += 1\n",
    "        return atoms\n",
    "    atoms1 = Molecular_formula(smiles1)\n",
    "    atoms2 = Molecular_formula(smiles2)\n",
    "    return atoms1 == atoms2\n",
    "    \n",
    "def analyze_result(list_name=\"evaluation.csv\",path=\"image_list\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    with open(list_name,\"r\") as f:\n",
    "        tp_list = [one.strip().split(\",\") for one in f.readlines()]\n",
    "    \n",
    "    print \"Number of data: %d\"% (len(tp_list)-1)\n",
    "        \n",
    "    true_list = [[i,one[0]] for i,one in enumerate(tp_list[1:]) if _re_smiles(one[0],one[1])]\n",
    "    print \"Number of matching: %d\" % (len(true_list))\n",
    "    print(true_list)\n",
    "    \n",
    "    true_list = [[i,one[0]] for i,one in enumerate(tp_list[1:]) if is_structural_isomer(one[0],one[1])]\n",
    "    print \"Number of matching: %d\" % (len(true_list))\n",
    "    print(true_list)\n",
    "        \n",
    "        \n",
    "    for i,smiles in enumerate(tp_list[1:]):\n",
    "        true_mol = Chem.MolFromSmiles(smiles[0])\n",
    "        pred_mol = Chem.MolFromSmiles(smiles[1])\n",
    "        image = Draw.MolsToImage([true_mol,pred_mol])\n",
    "        image.save(os.path.join(path,\"%d.png\" % i))\n",
    "    \n",
    "analyze_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtvae",
   "language": "python",
   "name": "jtvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
