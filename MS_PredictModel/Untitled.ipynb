{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training program for Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab,datasetのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of train dataset :', 6584)\n",
      "('number of validation dataset :', 732)\n"
     ]
    }
   ],
   "source": [
    "from fast_jtnn import *\n",
    "from MS_PredictModel import MS_Dataset,MS_Dataset_pickle,dataset_load\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(VOCAB_FILE,\"r\")]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "'''\n",
    "MS_Dataset.QUERY = \"\"\"select smiles,file_path from massbank where ms_type=\"MS\" and instrument_type=\"EI-B\" and smiles<>'N/A';\"\"\"\n",
    "dataset = MS_Dataset(vocab=vocab,host=\"localhost\",database=\"chemoinfo\",batch_size=20)\n",
    "'''\n",
    "train_vali_rate = 0.9\n",
    "\n",
    "train_dataset, vali_dataset = dataset_load(\"./massbank.pkl\",vocab,20,train_vali_rate)\n",
    "print(\"number of train dataset :\",len(train_dataset))\n",
    "print(\"number of validation dataset :\",len(vali_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/jtvae/lib/python2.7/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1027, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1027, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1027, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=47, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=142, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=57, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=146, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (0): Conv2d(1, 30, kernel_size=(3, 11), stride=(1, 1), padding=(1, 0))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(30, 30, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(30, 50, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(50, 50, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(50, 100, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(100, 100, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
      "    (11): ReLU()\n",
      "  )\n",
      "  (rnn): GRU(100, 100, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (T_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=200, out_features=28, bias=True)\n",
      ")\n",
      "Model #Params: 569K\n",
      "Model #Params: 395K\n"
     ]
    }
   ],
   "source": [
    "from ms_encoder import ms_peak_encoder,ms_peak_encoder_lstm,ms_peak_encoder_cnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "hidden_size = 100\n",
    "latent_size = 56\n",
    "depthT = 20\n",
    "depthG = 3\n",
    "\n",
    "dec_model = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG).to('cuda')\n",
    "print dec_model\n",
    "#enc_model = ms_peak_encoder_lstm(train_dataset.max_spectrum_size,output_size=latent_size,\\\n",
    "#        hidden_size=100,embedding_size=5,num_rnn_layers=2,bidirectional=True,dropout_rate=0.5).to('cuda')\n",
    "enc_model = ms_peak_encoder_cnn(train_dataset.max_spectrum_size,output_size=latent_size,\\\n",
    "                                 hidden_size=100,embedding_size=10,num_rnn_layers=2,bidirectional=True,dropout_rate=0.5).to('cuda')\n",
    "print enc_model\n",
    "\n",
    "for param in dec_model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)\n",
    "load_model = \"./vae_model/model.iter-50000\"\n",
    "dec_model.load_state_dict(torch.load(load_model,map_location='cuda'))\n",
    "\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in dec_model.parameters()]) / 1000,)\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in enc_model.parameters()]) / 1000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "optimizer = optim.Adam(enc_model.parameters(), lr=1e-3)\n",
    "#optimizer = optim.SGD(enc_model.parameters(),lr=100)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, 0.9)\n",
    "#scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch : ', 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1386: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/kusachi/anaconda3/envs/jtvae/lib/python2.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] , kl_loss 311.75, Word: 35.80, Topo: 85.28, Assm: 82.59 vali_Word: 33.66, vali_Topo: 85.42, vali_assm: 85.39, learning rate: 0.0010\n",
      "('epoch : ', 1)\n",
      "[400] , kl_loss 565.33, Word: 35.03, Topo: 85.93, Assm: 85.07 vali_Word: 34.72, vali_Topo: 85.19, vali_assm: 84.71, learning rate: 0.0010\n",
      "[600] , kl_loss 717.27, Word: 36.48, Topo: 86.26, Assm: 85.52 vali_Word: 38.79, vali_Topo: 85.85, vali_assm: 84.47, learning rate: 0.0010\n",
      "('epoch : ', 2)\n",
      "[800] , kl_loss 767.41, Word: 37.97, Topo: 86.42, Assm: 85.20 vali_Word: 41.30, vali_Topo: 85.88, vali_assm: 84.29, learning rate: 0.0010\n",
      "('epoch : ', 3)\n",
      "[1000] , kl_loss 773.03, Word: 40.91, Topo: 86.25, Assm: 85.19 vali_Word: 43.84, vali_Topo: 85.60, vali_assm: 85.08, learning rate: 0.0010\n",
      "[1200] , kl_loss 748.75, Word: 43.99, Topo: 86.57, Assm: 85.02 vali_Word: 44.63, vali_Topo: 85.91, vali_assm: 82.81, learning rate: 0.0010\n",
      "('epoch : ', 4)\n",
      "[1400] , kl_loss 767.15, Word: 45.69, Topo: 86.57, Assm: 85.50 vali_Word: 46.01, vali_Topo: 86.43, vali_assm: 85.93, learning rate: 0.0010\n",
      "[1600] , kl_loss 772.98, Word: 45.85, Topo: 86.90, Assm: 85.38 vali_Word: 47.55, vali_Topo: 86.34, vali_assm: 84.99, learning rate: 0.0010\n",
      "('epoch : ', 5)\n",
      "[1800] , kl_loss 788.24, Word: 46.32, Topo: 86.93, Assm: 85.88 vali_Word: 40.16, vali_Topo: 86.30, vali_assm: 83.93, learning rate: 0.0010\n",
      "('epoch : ', 6)\n",
      "[2000] , kl_loss 785.30, Word: 46.78, Topo: 86.85, Assm: 85.94 vali_Word: 43.91, vali_Topo: 86.26, vali_assm: 84.13, learning rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from MS_PredictModel import ms_peak_encoder,MS_Dataset\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "pbar = None\n",
    "train_dataset.batch_size = 20\n",
    "vali_dataset.batch_size = 10\n",
    "\n",
    "anneal_iter = 7400\n",
    "\n",
    "beta = 0\n",
    "step_beta = 0.02\n",
    "kl_anneal_iter = 10000\n",
    "max_beta = 1.0\n",
    "warmup = 20000\n",
    "\n",
    "def training(max_epoch = 100):\n",
    "    global pbar\n",
    "    global beta\n",
    "    total_step = 0\n",
    "    meters = np.zeros(4)\n",
    "    vali_meters = np.zeros(3)\n",
    "    with open(\"log2.csv\",\"w\") as f:\n",
    "        f.write(\"epoch,iter.,word,topo,assm,vali word,vali topo,vali assm\\n\")\n",
    "    for epoch in range(max_epoch):\n",
    "        print(\"epoch : \",epoch)\n",
    "        for batch in train_dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            total_step+=1\n",
    "            #pbar.update(1)\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            \n",
    "            enc_model.zero_grad()\n",
    "            dec_model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            h,kl_loss = enc_model(x,y,training=True,sample=True)\n",
    "            tree_vec = h[:,:h.shape[1]/2]\n",
    "            mol_vec  = h[:,h.shape[1]/2:]\n",
    "            _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "            word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "            assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "            total_loss = word_loss+topo_loss+assm_loss+beta*kl_loss\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            del x,y,h\n",
    "            \n",
    "            meters = meters + np.array([kl_loss.item(),word_acc * 100, topo_acc * 100, assm_acc * 100])\n",
    "            if total_step % 200 == 0:\n",
    "                vali_total = 0\n",
    "                for batch in vali_dataset:\n",
    "                    x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "                    x = x.to('cuda')\n",
    "                    y = y.to('cuda')\n",
    "                    with torch.no_grad():\n",
    "                        h,_ = enc_model(x,y,training=False,sample=False)\n",
    "                        tree_vec = h[:,:h.shape[1]/2]\n",
    "                        mol_vec  = h[:,h.shape[1]/2:]\n",
    "                        _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "                        word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "                        assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "                        vali_meters = vali_meters + np.array([word_acc * 100, topo_acc * 100, assm_acc * 100])\n",
    "                        vali_total += 1    \n",
    "                    del x,y,h\n",
    "                    \n",
    "                meters /= 200\n",
    "                vali_meters /= vali_total\n",
    "                print \"[%d] , kl_loss %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f vali_Word: %.2f, vali_Topo: %.2f, vali_assm: %.2f, learning rate: %.4f\" % \\\n",
    "                    (total_step, meters[0], meters[1], meters[2],meters[3], vali_meters[0],vali_meters[1],vali_meters[2],scheduler.get_lr()[0])\n",
    "                with open(\"log2.csv\",\"a\") as f:\n",
    "                    f.write(\"%d,%d,%.2f,%.2f,%.2f,%.2f,%.2f,%.2f\\n\" % (epoch,total_step,meters[0], meters[1], meters[2],vali_meters[0],vali_meters[1],vali_meters[2]))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "                vali_meters *= 0\n",
    "            if total_step % 200 == 0:\n",
    "                torch.save(enc_model.state_dict(), \"./enc_model\" + \"/model.iter-\" + str(total_step))\n",
    "            #if total_step % anneal_iter == 0:\n",
    "                #scheduler.step()\n",
    "                \n",
    "            if total_step % kl_anneal_iter == 0 and total_step >= warmup:\n",
    "                beta = min(max_beta, beta + step_beta)\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "try:\n",
    "    #if pbar is None:\n",
    "        #pbar = tqdm()\n",
    "    if not os.path.exists(\"./enc_model\"):\n",
    "        os.mkdir(\"./enc_model\")\n",
    "    training(500)\n",
    "except RuntimeError as e:\n",
    "    #if pbar is not None:\n",
    "        #del pbar\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    #import pdb; pdb.set_trace()\n",
    "    print(e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "model_path = None\n",
    "if model_path is not None:\n",
    "    enc_model.load_state_dict(torch.load(model_path,map_location='cuda'))\n",
    "\n",
    "train_dataset.batch_size = 20\n",
    "vali_dataset.batch_size = 10\n",
    "\n",
    "def evaluation():\n",
    "    ret = []\n",
    "    with torch.no_grad():\n",
    "        for batch in vali_dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            \n",
    "            h,_ = enc_model(x,y,training=False,sample=False)\n",
    "            tree_vec = h[:,:h.shape[1]/2]\n",
    "            mol_vec  = h[:,h.shape[1]/2:]\n",
    "            for num in range(h.size()[0]):\n",
    "                \n",
    "                true_smiles=x_batch[num].smiles\n",
    "                predict_smiles = dec_model.decode(tree_vec[num].view(1,latent_size/2),mol_vec[num].view(1,latent_size/2),False)\n",
    "                \n",
    "                #smilesの正規化\n",
    "                true_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(true_smiles),True)\n",
    "                predict_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(predict_smiles),True)\n",
    "                \n",
    "                ret.append((true_smiles,predict_smiles))\n",
    "    return ret\n",
    "result = evaluation()\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from PIL import ImageDraw,ImageFont\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "def _re_smiles(smiles1,smiles2):\n",
    "    #print(smiles1,smiles2)\n",
    "    smiles1 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles1),True)\n",
    "    smiles2 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles2),True)\n",
    "    return smiles1 == smiles2\n",
    "\n",
    "def is_structural_isomer(smiles1,smiles2):\n",
    "    def Molecular_formula(smiles):\n",
    "        atoms = {}\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        for atom in mol.GetAtoms():\n",
    "            if not atom.GetSymbol() in atoms:\n",
    "                atoms[atom.GetSymbol()] = 1\n",
    "            else:\n",
    "                atoms[atom.GetSymbol()] += 1\n",
    "        return atoms\n",
    "    atoms1 = Molecular_formula(smiles1)\n",
    "    atoms2 = Molecular_formula(smiles2)\n",
    "    return atoms1 == atoms2\n",
    "    \n",
    "def analyze_result(smiles_list,log_path=\"evaluation.csv\",path=\"image_list\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    with open(log_path,\"w\") as f:\n",
    "        print \"Number of data: %d\"% (len(smiles_list))\n",
    "        f.write(\"Number of data,%d\\n\" % (len(smiles_list)))\n",
    "        \n",
    "        true_list = [[i,one[0]] for i,one in enumerate(smiles_list) if _re_smiles(one[0],one[1])]\n",
    "        print \"Number of matching: %d\" % (len(true_list))\n",
    "        f.write(\"Number of matching: %d\\n\" % (len(true_list)))\n",
    "        print(true_list)\n",
    "    \n",
    "        true_list = [[i,one[0]] for i,one in enumerate(smiles_list) if is_structural_isomer(one[0],one[1]) and [i,one[0]] not in true_list]\n",
    "        print \"Number of matching: %d\" % (len(true_list))\n",
    "        f.write(\"Number of matching: %d\\n\" % (len(true_list)))\n",
    "        print(true_list)\n",
    "        \n",
    "        f.write(\"true,predict,ECFP-Tanimoto score,MACCS-Tanimoto score\\n\")\n",
    "        \n",
    "    for i,smiles in enumerate(smiles_list):\n",
    "        true_mol = Chem.MolFromSmiles(smiles[0])\n",
    "        predict_mol = Chem.MolFromSmiles(smiles[1])\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMorganFingerprint(true_mol,2)\n",
    "        predict_fingerprint = AllChem.GetMorganFingerprint(predict_mol,2)\n",
    "        ECFP_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMACCSKeysFingerprint(true_mol)\n",
    "        predict_fingerprint = AllChem.GetMACCSKeysFingerprint(predict_mol)\n",
    "        MACCS_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        \n",
    "        with open(log_path,\"a\") as f:\n",
    "            f.write(smiles[0]+\",\"+smiles[1]+\",\"+str(ECFP_score)+\",\"+str(MACCS_score)+\"\\n\")\n",
    "        \n",
    "        image = Draw.MolsToImage([true_mol,predict_mol])\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        font = ImageFont.load_default()\n",
    "        font.size=40\n",
    "        draw.text((0,0),str(ECFP_score)+\",\"+str(MACCS_score),(0, 0, 0),font=font)\n",
    "        image.save(os.path.join(path,\"%d.png\" % i))\n",
    "    \n",
    "analyze_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -f evaluation.csv ./image_list\n",
    "!zip -r image.zip ./image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtvae",
   "language": "python",
   "name": "jtvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
