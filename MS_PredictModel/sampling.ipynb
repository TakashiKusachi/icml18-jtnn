{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20190903-1742\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from fast_jtnn import *\n",
    "from MS_PredictModel import MS_Dataset,MS_Dataset_pickle,dataset_load\n",
    "import pickle\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "sys.path.append(os.path.split(os.getcwd())[0])\n",
    "\n",
    "def get_timef():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "print(get_timef())\n",
    "device=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test_vali_data.pkl', 732)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from MS_PredictModel import MS_Dataset,MS_Dataset_pickle,dataset_load\n",
    "\n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(VOCAB_FILE,\"r\")]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "vali_dataset_path = \"test_vali_data.pkl\"\n",
    "with open(vali_dataset_path,\"rb\") as f:\n",
    "    vali_data = pickle.load(f)\n",
    "print(vali_dataset_path,len(vali_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1027, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1027, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1027, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=47, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=142, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=57, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=146, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (0): Conv1d(11, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (rnn): GRU(64, 200, num_layers=2, batch_first=True)\n",
      "  (T_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (output): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n",
      "Model #Params: 569K\n",
      "Model #Params: 481K\n"
     ]
    }
   ],
   "source": [
    "from ms_encoder import ms_peak_encoder,ms_peak_encoder_lstm,ms_peak_encoder_cnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "hidden_size = 100\n",
    "latent_size = 56\n",
    "depthT = 20\n",
    "depthG = 3\n",
    "\n",
    "dec_model = JTNNVAE(vocab, hidden_size, latent_size, depthT, depthG).to(device)\n",
    "print dec_model\n",
    "#enc_model = ms_peak_encoder_lstm(train_dataset.max_spectrum_size,output_size=latent_size,\\\n",
    "#        hidden_size=200,embedding_size=20,num_rnn_layers=2,bidirectional=False,dropout_rate=0.5).to('cuda')\n",
    "enc_model = ms_peak_encoder_cnn(vali_data.max_spectrum_size,output_size=latent_size,\\\n",
    "                                 conv1_channel=64,conv2_channel=128,conv_output_channel=256,\\\n",
    "                                 kernel1_width=5,kernel2_width=5,conv_output_width=5,\\\n",
    "                                 hidden_size=200,embedding_size=10,num_rnn_layers=2,bidirectional=False,dropout_rate=0.2).to(device)\n",
    "print enc_model\n",
    "\n",
    "for param in dec_model.parameters():\n",
    "    if param.dim() == 1:\n",
    "        nn.init.constant_(param, 0)\n",
    "    else:\n",
    "        nn.init.xavier_normal_(param)\n",
    "\n",
    "model_path = \"./enc_model/model.iter-30000\"\n",
    "if model_path is not None:\n",
    "    enc_model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "model_path = \"./dec_model/model.iter-30000\"\n",
    "if model_path is not None:\n",
    "    dec_model.load_state_dict(torch.load(model_path,map_location=device))\n",
    "\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in dec_model.parameters()]) / 1000,)\n",
    "print \"Model #Params: %dK\" % (sum([x.nelement() for x in enc_model.parameters()]) / 1000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(torch.Size([10, 310, 10]),)\n",
      "(torch.Size([10, 310, 1]),)\n",
      "(torch.Size([10, 310, 11]),)\n",
      "(torch.Size([10, 310, 64]),)\n",
      "(torch.Size([10, 310, 64]),)\n",
      "(torch.Size([10, 310, 200]),)\n",
      "(torch.Size([10, 200]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [19:52, 17.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "vali_data.batch_size = 10\n",
    "\n",
    "sample_rate_list = [\n",
    "    #[0.0,1],\n",
    "    #[0.5,20],\n",
    "    [1.0,1]\n",
    "]\n",
    "\n",
    "def evaluation():\n",
    "    ret = []\n",
    "    with torch.no_grad():\n",
    "        for batch_number,batch in tqdm(enumerate(vali_data)):\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            result_one_data=[]\n",
    "            for x_data in x_batch:\n",
    "                result_one_data.append([Chem.MolToSmiles(Chem.MolFromSmiles(x_data.smiles),True)])\n",
    "            for sample_rate,times in sample_rate_list:\n",
    "                for No in range(times):\n",
    "                    h,_ = enc_model(x,y,training=False,sample=True,sample_rate=sample_rate)\n",
    "                    tree_vec = h[:,:h.shape[1]/2]\n",
    "                    mol_vec  = h[:,h.shape[1]/2:]\n",
    "                    for num in range(h.size()[0]):\n",
    "                        predict_smiles = dec_model.decode(tree_vec[num].view(1,latent_size/2),mol_vec[num].view(1,latent_size/2),False)\n",
    "                \n",
    "                        #smilesの正規化\n",
    "                        predict_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(predict_smiles),True)\n",
    "                \n",
    "                        result_one_data[num].append(predict_smiles)\n",
    "            ret.append(result_one_data)\n",
    "    return ret\n",
    "result = evaluation()\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = []\n",
    "for one in result:\n",
    "    ret.extend(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _re_smiles(smiles1,smiles2):\n",
    "    #print(smiles1,smiles2)\n",
    "    smiles1 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles1),True)\n",
    "    smiles2 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles2),True)\n",
    "    return smiles1 == smiles2\n",
    "\n",
    "def is_structural_isomer(smiles1,smiles2):\n",
    "    def Molecular_formula(smiles):\n",
    "        atoms = {}\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        for atom in mol.GetAtoms():\n",
    "            if not atom.GetSymbol() in atoms:\n",
    "                atoms[atom.GetSymbol()] = 1\n",
    "            else:\n",
    "                atoms[atom.GetSymbol()] += 1\n",
    "        return atoms\n",
    "    atoms1 = Molecular_formula(smiles1)\n",
    "    atoms2 = Molecular_formula(smiles2)\n",
    "    return atoms1 == atoms2\n",
    "\n",
    "def logout(one_sam,f):\n",
    "    global sample_rate_list\n",
    "    true_smiles = one_sam[0]\n",
    "    predict_smiles = one_sam[1:]\n",
    "    temp = []\n",
    "    cursor = 1\n",
    "    for one in one_sam:\n",
    "        f.write(one+\",\")\n",
    "    for one in predict_smiles:\n",
    "        f.write(str(_re_smiles(true_smiles,one))+\",\"+str(is_structural_isomer(true_smiles,one))+\",\")\n",
    "    ecfp_scores = []\n",
    "    maccs_scores = []\n",
    "    for one in predict_smiles:\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "        predict_mol = Chem.MolFromSmiles(one)\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMorganFingerprint(true_mol,2)\n",
    "        predict_fingerprint = AllChem.GetMorganFingerprint(predict_mol,2)\n",
    "        ECFP_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        ecfp_scores.append(ECFP_score)\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMACCSKeysFingerprint(true_mol)\n",
    "        predict_fingerprint = AllChem.GetMACCSKeysFingerprint(predict_mol)\n",
    "        MACCS_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        maccs_scores.append(MACCS_score)\n",
    "    for score in ecfp_scores:\n",
    "        f.write(str(score)+\",\")\n",
    "    f.write(\",\")\n",
    "    for score in maccs_scores:\n",
    "        f.write(str(score)+\",\")\n",
    "    f.write(\",\")\n",
    "        \n",
    "        \n",
    "    for rate,num in sample_rate_list:\n",
    "        temp.append(one_sam[cursor:cursor+num])\n",
    "        cursor += num\n",
    "    #print(temp)\n",
    "    \n",
    "    \n",
    "with open(\"area_sample.csv\",\"w\") as f:\n",
    "    f.write(\"{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,\\n\".format(get_timef()))\n",
    "    f.write(\"total sample,{}\\n\".format(len(ret)))\n",
    "    f.write(\"sample_rate,number of sample\\n\")\n",
    "    for rate,num in sample_rate_list:\n",
    "        f.write(\"{},{}\\n\".format(rate,num))\n",
    "    for one in ret:\n",
    "        logout(one,f)\n",
    "        f.write(\"\\n\")\n",
    "        pass\n",
    "    #logout(ret[0],f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jtvae",
   "language": "python",
   "name": "jtvae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
